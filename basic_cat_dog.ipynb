{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = '/home/administrator/Sidhanth/topo_fe/pickled_data/'\n",
    "\n",
    "def pickle_data(name,data,dir_name=dir_name):\n",
    "    pickle_file = dir_name + name\n",
    "    pickle_in = open(pickle_file,'wb')\n",
    "    pickle.dump(data,pickle_in)\n",
    "    pickle_in.close()\n",
    "    \n",
    "def pickle_load(pickle_file):\n",
    "    pickle_out = open(pickle_file,'rb')\n",
    "    data = pickle.load(pickle_out)\n",
    "    pickle_out.close()\n",
    "    return data\n",
    "    \n",
    "def load_pickle_file(name,img_size,dataset_path=None,dir_name=dir_name):\n",
    "    path_string = '_' + str(img_size) + '.pickle'\n",
    "    pickle_file = dir_name + name + path_string\n",
    "    \n",
    "    if os.path.exists(pickle_file):\n",
    "        data = pickle_load(pickle_file)\n",
    "        return data\n",
    "    \n",
    "    elif dataset_path is not None:\n",
    "        def load_data(dataset_path=dataset_path):\n",
    "            data = []\n",
    "            index = 0\n",
    "            for data_class in dataset_path:\n",
    "                data_paths = sorted(glob.glob(data_class))\n",
    "                class_num = dataset_path.index(data_class)\n",
    "                print(class_num)\n",
    "                count = 0\n",
    "                for data_path in tqdm(data_paths):\n",
    "                    try:\n",
    "                        img = cv2.imread(data_path,1)\n",
    "                        img = cv2.resize(img,(img_size,img_size),interpolation=cv2.INTER_CUBIC)\n",
    "                        data.append([np.asarray(img),class_num])\n",
    "                        count += 1\n",
    "                    except Exception as e:\n",
    "                        print(path+':Corrupt file')\n",
    "                        pass\n",
    "                print('Class '+str(class_num)+':'+str(count))\n",
    "                index += 1\n",
    "            return data\n",
    "        \n",
    "        dataset = load_data()\n",
    "        random.shuffle(dataset)\n",
    "        x = []\n",
    "        y = []\n",
    "        \n",
    "        for images,labels in dataset:\n",
    "            x.append(images)\n",
    "            y.append(labels)\n",
    "            \n",
    "        x,y = np.array(x),np.array(y)\n",
    "        print(x.shape,y.shape)\n",
    "        pickle_data('x' + path_string,x)\n",
    "        pickle_data('y' + path_string,y)\n",
    "        data = pickle_load(pickle_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:Loss 1 34.76924389600754\n",
      "Epoch:Loss 6 34.65751761198044\n",
      "Epoch:Loss 11 34.65745663642883\n",
      "Epoch:Loss 16 34.65742415189743\n",
      "Epoch:Loss 21 34.65740633010864\n",
      "Epoch:Loss 26 34.657394766807556\n",
      "Epoch:Loss 31 34.65738445520401\n",
      "Epoch:Loss 36 34.657374799251556\n",
      "Epoch:Loss 41 34.65736651420593\n",
      "Epoch:Loss 46 34.65736025571823\n",
      "Epoch:Loss 51 34.65735387802124\n",
      "Epoch:Loss 56 34.65734785795212\n",
      "Epoch:Loss 61 34.65734225511551\n",
      "Epoch:Loss 66 34.65733689069748\n",
      "Epoch:Loss 71 34.65733140707016\n",
      "Epoch:Loss 76 34.65732669830322\n",
      "Epoch:Loss 81 34.65732169151306\n",
      "Epoch:Loss 86 34.657316863536835\n",
      "Epoch:Loss 91 34.65731346607208\n",
      "Epoch:Loss 96 34.657310009002686\n",
      "Epoch:Loss 101 34.65730655193329\n",
      "Epoch:Loss 106 34.657303750514984\n",
      "Epoch:Loss 111 34.657301008701324\n",
      "Epoch:Loss 116 34.65729856491089\n",
      "Epoch:Loss 121 34.65729594230652\n",
      "Epoch:Loss 126 34.657293140888214\n",
      "Epoch:Loss 131 34.65729033946991\n",
      "Epoch:Loss 136 34.657287776470184\n",
      "Epoch:Loss 141 34.657285153865814\n",
      "Epoch:Loss 146 34.657282531261444\n",
      "Epoch:Loss 151 34.65727984905243\n",
      "Epoch:Loss 156 34.65727734565735\n",
      "Epoch:Loss 161 34.65727472305298\n",
      "Epoch:Loss 166 34.65727227926254\n",
      "Epoch:Loss 171 34.65726947784424\n",
      "Epoch:Loss 176 34.65726691484451\n",
      "Epoch:Loss 181 34.65726447105408\n",
      "Epoch:Loss 186 34.65726178884506\n",
      "Epoch:Loss 191 34.65725922584534\n",
      "Epoch:Loss 196 34.6572567820549\n",
      "Epoch:Loss 201 34.657254219055176\n",
      "Epoch:Loss 206 34.65725165605545\n",
      "Epoch:Loss 211 34.65724939107895\n",
      "Epoch:Loss 216 34.65724694728851\n",
      "Epoch:Loss 221 34.65724468231201\n",
      "Epoch:Loss 226 34.65724241733551\n",
      "Epoch:Loss 231 34.65723991394043\n",
      "Epoch:Loss 236 34.65723770856857\n",
      "Epoch:Loss 241 34.65723520517349\n",
      "Epoch:Loss 246 34.65723305940628\n",
      "Epoch:Loss 251 34.657230734825134\n",
      "Epoch:Loss 256 34.65722841024399\n",
      "Epoch:Loss 261 34.6572260260582\n",
      "Epoch:Loss 266 34.65722346305847\n",
      "Epoch:Loss 271 34.657221138477325\n",
      "Epoch:Loss 276 34.65721899271011\n",
      "Epoch:Loss 281 34.65721660852432\n",
      "Epoch:Loss 286 34.657214403152466\n",
      "Epoch:Loss 291 34.657212257385254\n",
      "Epoch:Loss 296 34.65720987319946\n",
      "Epoch:Loss 301 34.65720748901367\n",
      "Epoch:Loss 306 34.657205283641815\n",
      "Epoch:Loss 311 34.65720325708389\n",
      "Epoch:Loss 316 34.65720093250275\n",
      "Epoch:Loss 321 34.657198548316956\n",
      "Epoch:Loss 326 34.657196044921875\n",
      "Epoch:Loss 331 34.657193660736084\n",
      "Epoch:Loss 336 34.657191157341\n",
      "Epoch:Loss 341 34.6571888923645\n",
      "Epoch:Loss 346 34.65718638896942\n",
      "Epoch:Loss 351 34.657184064388275\n",
      "Epoch:Loss 356 34.65718138217926\n",
      "Epoch:Loss 361 34.65717899799347\n",
      "Epoch:Loss 366 34.65717667341232\n",
      "Epoch:Loss 371 34.65717440843582\n",
      "Epoch:Loss 376 34.65717190504074\n",
      "Epoch:Loss 381 34.657169580459595\n",
      "Epoch:Loss 386 34.65716713666916\n",
      "Epoch:Loss 391 34.6571649312973\n",
      "Epoch:Loss 396 34.657162606716156\n",
      "Epoch:Loss 401 34.657160103321075\n",
      "Epoch:Loss 406 34.65715795755386\n",
      "Epoch:Loss 411 34.65715551376343\n",
      "Epoch:Loss 416 34.65715253353119\n",
      "Epoch:Loss 421 34.657149851322174\n",
      "Epoch:Loss 426 34.657146871089935\n",
      "Epoch:Loss 431 34.657144010066986\n",
      "Epoch:Loss 436 34.65714156627655\n",
      "Epoch:Loss 441 34.65713834762573\n",
      "Epoch:Loss 446 34.65713554620743\n",
      "Epoch:Loss 451 34.65713268518448\n",
      "Epoch:Loss 456 34.65713006258011\n",
      "Epoch:Loss 461 34.657127022743225\n",
      "Epoch:Loss 466 34.657124161720276\n",
      "Epoch:Loss 471 34.65712136030197\n",
      "Epoch:Loss 476 34.65711855888367\n",
      "Epoch:Loss 481 34.65711581707001\n",
      "Epoch:Loss 486 34.65711289644241\n",
      "Epoch:Loss 491 34.65711027383804\n",
      "Epoch:Loss 496 34.65710753202438\n"
     ]
    }
   ],
   "source": [
    "html_path = '/home/administrator/Sidhanth/topo_fe/mapper_outputs/'\n",
    "path = '/home/administrator/Sidhanth/Datasets/cat_dog_dataset/training_set/'\n",
    "cat_dataset_path = path + 'cats/cat.*.jpg'\n",
    "dog_dataset_path = path + 'dogs/dog.*.jpg'\n",
    "dataset_path = [cat_dataset_path,dog_dataset_path]\n",
    "\n",
    "img_size = 32\n",
    "\n",
    "x = load_pickle_file('x',img_size,dataset_path=dataset_path)\n",
    "y = load_pickle_file('y',img_size)\n",
    "x = np.moveaxis(x,-1,1)\n",
    "\n",
    "x_train,x_test = x[:6400],x[6400:] \n",
    "y_train,y_test = y[:6400],y[6400:]\n",
    "\n",
    "#x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
    "#print(x.shape,y.shape,x_train.shape,y_train.shape)\n",
    "x_,y_ = torch.from_numpy(x_train).float().cuda(),torch.from_numpy(y_train).float().cuda()\n",
    "\n",
    "\n",
    "class cat_dog_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 3, 32, 32)\n",
    "        xb = F.leaky_relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        \n",
    "        xb = F.leaky_relu(self.conv3(xb))\n",
    "        xb = F.relu(self.conv4(xb))\n",
    "        \n",
    "        xb = F.leaky_relu(self.conv5(xb))\n",
    "        xb = F.relu(self.conv6(xb))\n",
    "        \n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        xb = F.sigmoid(xb)\n",
    "        return xb.view(-1)\n",
    "    \n",
    "model = cat_dog_CNN().cuda()\n",
    "criterion_bce = nn.BCELoss()\n",
    "opt = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    \n",
    "    #with tqdm(total=8000/batch_size) as pbar:\n",
    "    while end_index < 6400:\n",
    "        end_index = start_index + batch_size\n",
    "        image = x_[start_index:end_index]\n",
    "        pred = model(image)\n",
    "        label = y_[start_index:end_index]\n",
    "        model.zero_grad()\n",
    "        loss = criterion_bce(pred,label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        #pbar.set_postfix_str(s='Loss:'+str(loss.item()))\n",
    "        #pbar.update()\n",
    "        start_index = end_index\n",
    "    if epoch%5==0:\n",
    "        print('Epoch:Loss',epoch+1,epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t,y_t = torch.from_numpy(x_test).float().cuda(),torch.from_numpy(y_test).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x_t).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5002766 , 0.50007164, 0.5008787 , 0.50040317], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[y_pred>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
